/* tslint:disable */
/* eslint-disable */
/**
 * Hermes API
 * Canonical Hermes OpenAPI contract for Project LOGOS. See Project LOGOS spec: Table 2 in Section 3.4 (Hermes API endpoints).  Hermes is the stateless language & embedding utility providing: - Speech-to-text (STT) - Text-to-speech (TTS) - Simple NLP preprocessing - Text embedding generation - LLM gateway proxy via `/llm`  All endpoints are stateless and do not interact with the HCG directly. 
 *
 * The version of the OpenAPI document: 1.0.0
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */

import { mapValues } from '../runtime';
import type { LLMMessage } from './LLMMessage';
import {
    LLMMessageFromJSON,
    LLMMessageFromJSONTyped,
    LLMMessageToJSON,
    LLMMessageToJSONTyped,
} from './LLMMessage';

/**
 * Request payload for Hermes LLM gateway.
 * @export
 * @interface LLMRequest
 */
export interface LLMRequest {
    /**
     * Shortcut for a single user message when `messages` is omitted.
     * @type {string}
     * @memberof LLMRequest
     */
    prompt?: string;
    /**
     * Conversation history forwarded to the provider.
     * @type {Array<LLMMessage>}
     * @memberof LLMRequest
     */
    messages?: Array<LLMMessage>;
    /**
     * Override the configured provider (e.g., `openai`, `echo`, `local`).
     * @type {string}
     * @memberof LLMRequest
     */
    provider?: string;
    /**
     * Provider-specific model identifier override.
     * @type {string}
     * @memberof LLMRequest
     */
    model?: string;
    /**
     * Sampling temperature forwarded to the provider.
     * @type {number}
     * @memberof LLMRequest
     */
    temperature?: number;
    /**
     * Optional maximum number of tokens to generate.
     * @type {number}
     * @memberof LLMRequest
     */
    maxTokens?: number;
    /**
     * Additional metadata stored alongside the request.
     * @type {{ [key: string]: any; }}
     * @memberof LLMRequest
     */
    metadata?: { [key: string]: any; };
}

/**
 * Check if a given object implements the LLMRequest interface.
 */
export function instanceOfLLMRequest(value: object): value is LLMRequest {
    return true;
}

export function LLMRequestFromJSON(json: any): LLMRequest {
    return LLMRequestFromJSONTyped(json, false);
}

export function LLMRequestFromJSONTyped(json: any, ignoreDiscriminator: boolean): LLMRequest {
    if (json == null) {
        return json;
    }
    return {
        
        'prompt': json['prompt'] == null ? undefined : json['prompt'],
        'messages': json['messages'] == null ? undefined : ((json['messages'] as Array<any>).map(LLMMessageFromJSON)),
        'provider': json['provider'] == null ? undefined : json['provider'],
        'model': json['model'] == null ? undefined : json['model'],
        'temperature': json['temperature'] == null ? undefined : json['temperature'],
        'maxTokens': json['max_tokens'] == null ? undefined : json['max_tokens'],
        'metadata': json['metadata'] == null ? undefined : json['metadata'],
    };
}

export function LLMRequestToJSON(json: any): LLMRequest {
    return LLMRequestToJSONTyped(json, false);
}

export function LLMRequestToJSONTyped(value?: LLMRequest | null, ignoreDiscriminator: boolean = false): any {
    if (value == null) {
        return value;
    }

    return {
        
        'prompt': value['prompt'],
        'messages': value['messages'] == null ? undefined : ((value['messages'] as Array<any>).map(LLMMessageToJSON)),
        'provider': value['provider'],
        'model': value['model'],
        'temperature': value['temperature'],
        'max_tokens': value['maxTokens'],
        'metadata': value['metadata'],
    };
}

