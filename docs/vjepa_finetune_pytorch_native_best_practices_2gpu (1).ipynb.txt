{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# V-JEPA2 → CLIP Alignment (PyTorch-Native Rewrite)\n",
        "\n",
        "A clean rewrite that embraces PyTorch's internal data structures and vectorization:\n",
        "\n",
        "- **PEFT LoRA** instead of custom LoRA implementation\n",
        "- **Pre-allocated circular buffer** for memory bank (no Python list append/pop)\n",
        "- **Batch video decoding** with decord's native batch API\n",
        "- **torch.compile** on the projection head\n",
        "- **Native cross-device autograd** (no custom Function needed)\n",
        "- **Vectorized contrastive loss** with einsum\n",
        "- **Proper DataLoader workers** with prefetching\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "installs",
      "metadata": {
        "id": "installs",
        "outputId": "8d812534-f7b3-4150-de4b-cb565c45d32c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q peft accelerate transformers decord wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d428a147-d817-4902-a07d-0d7bb5bcdd46",
      "metadata": {
        "id": "d428a147-d817-4902-a07d-0d7bb5bcdd46"
      },
      "outputs": [],
      "source": [
        "#import torch\n",
        "\n",
        "#torch.set_float32_matmul_precision('high')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {
        "id": "imports",
        "outputId": "ccee5988-df91-4b3e-84cb-33b3a6f458d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: 2.4.1+cu124\n",
            "CUDA available: True\n",
            "GPU count: 2\n",
            "  GPU0: NVIDIA A40\n",
            "  GPU1: NVIDIA A40\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import AutoModel, AutoVideoProcessor, CLIPModel, CLIPProcessor\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import wandb\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "for i in range(torch.cuda.device_count()):\n",
        "    print(f\"  GPU{i}: {torch.cuda.get_device_name(i)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "config-header",
      "metadata": {
        "id": "config-header"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "Using a dataclass for type safety and IDE support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "config",
      "metadata": {
        "id": "config",
        "outputId": "dfa3241b-4ede-490f-b941-3520ff520f6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config(vjepa_model='facebook/vjepa2-vitl-fpc64-256', clip_model='openai/clip-vit-large-patch14', lora_r=12, lora_alpha=24, lora_dropout=0.05, lora_target_modules=['query', 'key', 'value'], num_frames=32, window_stride=4, max_windows_per_video=256, clip_sample_frames=8, video_dir='./videos', num_videos=None, test_split=0.15, batch_size=64, epochs=100, weight_decay=0.0001, warmup_ratio=0.2, grad_clip=1.0, temperature=0.3, memory_bank_size=256, lambda_align=0.0, proj_hidden_dim=1024, proj_warmup_steps=250, seed=42, num_workers=8, use_compile=False, use_bf16=True, gradient_checkpointing=True, use_wandb=True, wandb_project='vjepa-clip-alignment', wandb_run_name=None, vjepa_device='cuda:0', clip_device='cuda:1')\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    # Models\n",
        "    vjepa_model: str = \"facebook/vjepa2-vitl-fpc64-256\"\n",
        "    clip_model: str = \"openai/clip-vit-large-patch14\"\n",
        "\n",
        "    # LoRA (PEFT)\n",
        "    lora_r: int = 12\n",
        "    lora_alpha: int = 24\n",
        "    lora_dropout: float = 0.05\n",
        "    lora_target_modules: list = field(default_factory=lambda: [\"query\", \"key\", \"value\"])\n",
        "\n",
        "    # learning rates\n",
        "    lr_proj = 0.03\n",
        "    lr_vjepa = 0.003\n",
        "    # Video sampling\n",
        "    num_frames: int = 32\n",
        "    window_stride: int = 4\n",
        "    max_windows_per_video: int = 256\n",
        "    clip_sample_frames: int = 8\n",
        "\n",
        "    # Data\n",
        "    video_dir: str = \"./videos\"\n",
        "    num_videos: Optional[int] = None  # None = use all\n",
        "    test_split: float = 0.15\n",
        "\n",
        "    # Training\n",
        "    batch_size: int = 64\n",
        "    epochs: int = 100\n",
        "    #lr: float = 5e-5  # Lower LR for stability\n",
        "    weight_decay: float = 0.0001\n",
        "    warmup_ratio: float = 0.2\n",
        "    grad_clip: float = 1.0  # Tighter gradient clipping\n",
        "\n",
        "    # Contrastive learning\n",
        "    temperature: float = 0.3\n",
        "    memory_bank_size: int = 256  # Total embeddings to keep\n",
        "    lambda_align: float = 0.0    # 0-1 weight for cosine importance in loss calculation\n",
        "\n",
        "    # Projection head\n",
        "    proj_hidden_dim: int = 1024\n",
        "    proj_warmup_steps: int = 250  # Train only projection head for this many steps\n",
        "\n",
        "    # System\n",
        "    seed: int = 42\n",
        "    num_workers: int = 8\n",
        "    use_compile: bool = False  # torch.compile can cause NaN issues, disable by default\n",
        "    use_bf16: bool = True\n",
        "    gradient_checkpointing: bool = True\n",
        "\n",
        "    # Wandb logging\n",
        "    use_wandb: bool = True\n",
        "    wandb_project: str = \"vjepa-clip-alignment\"\n",
        "    wandb_run_name: Optional[str] = None  # Auto-generated if None\n",
        "\n",
        "    # Devices (auto-configured)\n",
        "    vjepa_device: str = \"cuda:0\"\n",
        "    clip_device: str = \"cuda:1\" if torch.cuda.device_count() > 1 else \"cuda:0\"\n",
        "\n",
        "cfg = Config()\n",
        "print(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "memory-bank-header",
      "metadata": {
        "id": "memory-bank-header"
      },
      "source": [
        "## Circular Memory Bank (Vectorized)\n",
        "\n",
        "Pre-allocated tensor with O(1) insert via index tracking. No Python list operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "memory-bank",
      "metadata": {
        "id": "memory-bank"
      },
      "outputs": [],
      "source": [
        "class CircularMemoryBank(nn.Module):\n",
        "    \"\"\"Pre-allocated circular buffer for contrastive learning.\n",
        "\n",
        "    All operations are vectorized tensor ops - no Python loops or list appends.\n",
        "    Uses scatter_ for clean wraparound handling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, capacity: int, embed_dim: int, device: torch.device):\n",
        "        super().__init__()\n",
        "        self.capacity = capacity\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Pre-allocate the buffer (not a parameter, just persistent state)\n",
        "        self.register_buffer('bank', torch.zeros(capacity, embed_dim, device=device))\n",
        "        self.register_buffer('ptr', torch.zeros(1, dtype=torch.long, device=device))\n",
        "        self.register_buffer('count', torch.zeros(1, dtype=torch.long, device=device))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def push(self, embeddings: torch.Tensor) -> None:\n",
        "        \"\"\"Add embeddings to the bank. embeddings: [B, D]\"\"\"\n",
        "        B = embeddings.shape[0]\n",
        "\n",
        "        # Normalize before storing\n",
        "        embeddings = F.normalize(embeddings.float(), dim=-1)\n",
        "\n",
        "        # Calculate write indices with modular arithmetic (handles wraparound)\n",
        "        ptr = self.ptr.item()\n",
        "        indices = (torch.arange(B, device=embeddings.device) + ptr) % self.capacity\n",
        "\n",
        "        # Scatter write - handles wraparound automatically\n",
        "        self.bank.index_copy_(0, indices, embeddings)\n",
        "\n",
        "        # Update pointer and count\n",
        "        self.ptr[0] = (ptr + B) % self.capacity\n",
        "        self.count[0] = min(self.count.item() + B, self.capacity)\n",
        "\n",
        "    def get_all(self) -> torch.Tensor:\n",
        "        \"\"\"Get all valid embeddings in the bank.\"\"\"\n",
        "        count = self.count.item()\n",
        "        if count == 0:\n",
        "            return torch.zeros(0, self.embed_dim, device=self.bank.device)\n",
        "        return self.bank[:count].clone()\n",
        "\n",
        "    @property\n",
        "    def size(self) -> int:\n",
        "        return self.count.item()\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        self.ptr.zero_()\n",
        "        self.count.zero_()\n",
        "        self.bank.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "loss-header",
      "metadata": {
        "id": "loss-header"
      },
      "source": [
        "## Vectorized Contrastive Loss\n",
        "\n",
        "Using einsum for clarity and letting PyTorch optimize the underlying ops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "loss",
      "metadata": {
        "id": "loss"
      },
      "outputs": [],
      "source": [
        "class InfoNCELoss(nn.Module):\n",
        "    \"\"\"Combined contrastive + alignment loss for embedding translation.\n",
        "\n",
        "    Combines InfoNCE (distinguishes embeddings) with cosine alignment (pulls pairs together).\n",
        "    Includes extensive numerical stability fixes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, temperature: float = 0.07, lambda_align: float = 1.0):\n",
        "        super().__init__()\n",
        "        # Clamp temperature to prevent numerical issues\n",
        "        self.temperature = max(temperature, 0.01)\n",
        "        self.lambda_align = lambda_align\n",
        "        self.eps = 1e-8\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        queries: torch.Tensor,      # [B, D] - projected V-JEPA embeddings\n",
        "        keys: torch.Tensor,         # [B, D] - CLIP embeddings for this batch\n",
        "        memory: torch.Tensor,       # [M, D] - memory bank embeddings (can be empty)\n",
        "    ) -> dict[str, torch.Tensor]:\n",
        "        \"\"\"Compute combined loss with numerical stability.\n",
        "\n",
        "        Returns dict with:\n",
        "            - loss: combined weighted loss (for backward)\n",
        "            - contrastive: InfoNCE component (for logging)\n",
        "            - alignment: cosine alignment component (for logging)\n",
        "            - acc: contrastive accuracy\n",
        "            - cos: mean cosine similarity\n",
        "        \"\"\"\n",
        "        B = queries.shape[0]\n",
        "        device = queries.device\n",
        "\n",
        "        # Force fp32 for all loss computation (critical for stability)\n",
        "        queries = queries.float()\n",
        "        keys = keys.float()\n",
        "        if memory.numel() > 0:\n",
        "            memory = memory.float()\n",
        "\n",
        "        # Normalize with eps for numerical stability\n",
        "        q = F.normalize(queries, dim=-1, eps=self.eps)  # [B, D]\n",
        "        k = F.normalize(keys, dim=-1, eps=self.eps)     # [B, D]\n",
        "\n",
        "        # Check for NaNs/Infs in inputs\n",
        "        #if not torch.isfinite(q).all():\n",
        "        #    print(f\"[WARN] Non-finite values in normalized queries\")\n",
        "        #    q = torch.nan_to_num(q, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "        #if not torch.isfinite(k).all():\n",
        "        #    print(f\"[WARN] Non-finite values in normalized keys\")\n",
        "        #    k = torch.nan_to_num(k, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "\n",
        "        # === Alignment loss (pulls paired embeddings together) ===\n",
        "        # Clamp cosine similarity to valid range [-1, 1]\n",
        "        cos_sim = (q * k).sum(dim=-1)  # [B] - more stable than einsum for element-wise\n",
        "        cos_sim = torch.clamp(cos_sim, -1.0 + self.eps, 1.0 - self.eps)\n",
        "        align_loss = (1.0 - cos_sim).mean()\n",
        "\n",
        "        # === Contrastive loss (distinguishes from negatives) ===\n",
        "        M = memory.shape[0] if memory.numel() > 0 else 0\n",
        "\n",
        "        if M > 0:\n",
        "            # Normalize memory with same eps\n",
        "            memory = F.normalize(memory, dim=-1, eps=self.eps)\n",
        "            if not torch.isfinite(memory).all():\n",
        "                print(f\"[WARN] Non-finite values in memory bank\")\n",
        "                memory = torch.nan_to_num(memory, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "            all_keys = torch.cat([memory, k], dim=0)  # [M+B, D]\n",
        "        else:\n",
        "            all_keys = k  # [B, D]\n",
        "\n",
        "        # Compute logits with temperature scaling\n",
        "        # Using @ instead of einsum for potentially better numerical properties\n",
        "        logits = (q @ all_keys.T) / self.temperature  # [B, M+B]\n",
        "\n",
        "        # Clamp logits to prevent overflow in softmax\n",
        "        # exp(85) is near float32 max, so we clamp to [-50, 50] to be safe\n",
        "        logits = torch.clamp(logits, -50.0, 50.0)\n",
        "\n",
        "        # Check for NaNs in logits\n",
        "        if not torch.isfinite(logits).all():\n",
        "            print(f\"[WARN] Non-finite logits detected, clamping\")\n",
        "            logits = torch.nan_to_num(logits, nan=0.0, posinf=50.0, neginf=-50.0)\n",
        "\n",
        "        labels = torch.arange(M, M + B, device=device, dtype=torch.long)\n",
        "\n",
        "        # Use label smoothing for additional stability\n",
        "        contrastive_loss = F.cross_entropy(logits, labels, label_smoothing=0.0)\n",
        "\n",
        "        # Final NaN check on losses\n",
        "        if (not torch.isfinite(contrastive_loss)) or (not torch.isfinite(align_loss)):\n",
        "            return {\n",
        "                'loss': None,\n",
        "                'skip': True,\n",
        "                'contrastive': torch.tensor(float('nan'), device=device),\n",
        "                'alignment': torch.tensor(float('nan'), device=device),\n",
        "                'acc': torch.tensor(0.0, device=device),\n",
        "                'cos': cos_sim.mean().detach(),\n",
        "            }\n",
        "        # === Combined loss ===\n",
        "        total_loss = contrastive_loss + self.lambda_align * align_loss\n",
        "\n",
        "        # === Metrics (no grad) ===\n",
        "        with torch.no_grad():\n",
        "            # Safe accuracy computation\n",
        "            if torch.isfinite(logits).all():\n",
        "                acc = (logits.argmax(dim=1) == labels).float().mean()\n",
        "            else:\n",
        "                acc = torch.tensor(0.0, device=device)\n",
        "\n",
        "        return {\n",
        "            'loss': total_loss,\n",
        "            'contrastive': contrastive_loss.detach(),\n",
        "            'alignment': align_loss.detach(),\n",
        "            'acc': acc,\n",
        "            'cos': cos_sim.mean().detach(),\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "projection-header",
      "metadata": {
        "id": "projection-header"
      },
      "source": [
        "## Projection Head\n",
        "\n",
        "Simple MLP, optionally compiled with `torch.compile`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "projection",
      "metadata": {
        "id": "projection"
      },
      "outputs": [],
      "source": [
        "class ProjectionHead(nn.Module):\n",
        "    \"\"\"MLP to project V-JEPA embeddings to CLIP space.\"\"\"\n",
        "\n",
        "    def __init__(self, in_dim: int, out_dim: int, hidden_dim: int = 1024):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.zeros_(self.fc1.bias)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.float()\n",
        "\n",
        "        # Check input\n",
        "        if not torch.isfinite(x).all():\n",
        "            print(f\"[PROJ] Input has NaN/Inf\")\n",
        "            x = torch.where(torch.isfinite(x), x, torch.zeros_like(x))\n",
        "\n",
        "        # Manual normalization\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        x = (x - mean) / (var.sqrt() + 1e-6)\n",
        "\n",
        "        # Check weights - if corrupted, skip this forward but keep gradient\n",
        "        if not torch.isfinite(self.fc1.weight).all():\n",
        "            print(f\"[PROJ] fc1.weight has NaN\")\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "\n",
        "        if not torch.isfinite(self.fc2.weight).all():\n",
        "            print(f\"[PROJ] fc2.weight has NaN\")\n",
        "\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # Safe L2 normalize - handle zero vectors\n",
        "        norm = x.norm(dim=-1, keepdim=True).clamp(min=1e-6)\n",
        "        x = x / norm\n",
        "\n",
        "        # Replace any NaN with zeros while keeping gradient\n",
        "        if not torch.isfinite(x).all():\n",
        "            print(f\"[PROJ] Output has NaN after normalize\")\n",
        "            x = torch.where(torch.isfinite(x), x, torch.zeros_like(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dataset-header",
      "metadata": {
        "id": "dataset-header"
      },
      "source": [
        "## Dataset with Batch Video Decoding\n",
        "\n",
        "Using decord's batch API and pre-building the full index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dataset",
      "metadata": {
        "id": "dataset"
      },
      "outputs": [],
      "source": [
        "from decord import VideoReader, cpu\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='decord')\n",
        "\n",
        "\n",
        "def build_window_index(video_dir: Path, cfg: Config) -> list[tuple[Path, int, int]]:\n",
        "    \"\"\"Build index of (video_path, start_frame, total_frames) tuples.\"\"\"\n",
        "    video_exts = {\".mp4\", \".avi\", \".mov\", \".mkv\", \".webm\"}\n",
        "    video_files = sorted([f for f in video_dir.iterdir() if f.suffix.lower() in video_exts])\n",
        "\n",
        "    if cfg.num_videos:\n",
        "        video_files = video_files[:cfg.num_videos]\n",
        "\n",
        "    index = []\n",
        "    for vpath in video_files:\n",
        "        try:\n",
        "            vr = VideoReader(str(vpath), ctx=cpu(0))\n",
        "            total = len(vr)\n",
        "            del vr\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        if total < cfg.num_frames:\n",
        "            continue\n",
        "\n",
        "        # Generate window start positions\n",
        "        max_start = total - cfg.num_frames\n",
        "        starts = list(range(0, max_start + 1, cfg.window_stride))[:cfg.max_windows_per_video]\n",
        "\n",
        "        for start in starts:\n",
        "            index.append((vpath, start, total))\n",
        "\n",
        "    return index\n",
        "\n",
        "\n",
        "class VideoWindowDataset(Dataset):\n",
        "    \"\"\"Dataset that loads video windows and precomputed CLIP embeddings.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        index: list[tuple[Path, int, int]],\n",
        "        processor: AutoVideoProcessor,\n",
        "        clip_cache: dict[tuple[str, int], torch.Tensor],\n",
        "        num_frames: int,\n",
        "    ):\n",
        "        self.index = index\n",
        "        self.processor = processor\n",
        "        self.clip_cache = clip_cache\n",
        "        self.num_frames = num_frames\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.index)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> dict:\n",
        "        vpath, start, total = self.index[idx]\n",
        "\n",
        "        # Load frames using decord's batch API\n",
        "        vr = VideoReader(str(vpath), ctx=cpu(0))\n",
        "        frame_indices = list(range(start, min(start + self.num_frames, total)))\n",
        "\n",
        "        # Pad if needed\n",
        "        while len(frame_indices) < self.num_frames:\n",
        "            frame_indices.append(frame_indices[-1])\n",
        "\n",
        "        # Batch decode - much faster than individual frame access\n",
        "        frames = vr.get_batch(frame_indices).asnumpy()  # [T, H, W, C]\n",
        "        del vr\n",
        "\n",
        "        # Process for V-JEPA\n",
        "        inputs = self.processor(frames, return_tensors=\"pt\")\n",
        "        pixel_values = inputs['pixel_values_videos'].squeeze(0)  # [T, C, H, W]\n",
        "\n",
        "        # Get precomputed CLIP embedding\n",
        "        clip_emb = self.clip_cache[(str(vpath), start)]\n",
        "\n",
        "        return {\n",
        "            'pixel_values': pixel_values,\n",
        "            'clip_embedding': clip_emb,\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(batch: list[dict]) -> dict:\n",
        "    \"\"\"Stack batch items into tensors.\"\"\"\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "        'clip_embedding': torch.stack([x['clip_embedding'] for x in batch]),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "precompute-header",
      "metadata": {
        "id": "precompute-header"
      },
      "source": [
        "## Precompute CLIP Embeddings\n",
        "\n",
        "Batch processing with proper GPU utilization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "precompute",
      "metadata": {
        "id": "precompute"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def precompute_clip_embeddings(\n",
        "    index: list[tuple[Path, int, int]],\n",
        "    clip_model: CLIPModel,\n",
        "    clip_processor: CLIPProcessor,\n",
        "    cfg: Config,\n",
        "    batch_size: int = 64,  # Process multiple windows at once\n",
        ") -> dict[tuple[str, int], torch.Tensor]:\n",
        "    \"\"\"Precompute CLIP embeddings for all windows with batching.\"\"\"\n",
        "    from collections import defaultdict\n",
        "    from tqdm.auto import tqdm\n",
        "\n",
        "    device = next(clip_model.parameters()).device\n",
        "    clip_model.eval()\n",
        "\n",
        "    # Group windows by video for efficient loading\n",
        "    windows_by_video = defaultdict(list)\n",
        "    for vpath, start, total in index:\n",
        "        windows_by_video[vpath].append((start, total))\n",
        "\n",
        "    cache = {}\n",
        "\n",
        "    for vpath, windows in tqdm(windows_by_video.items(), desc=\"Precomputing CLIP\"):\n",
        "        vr = VideoReader(str(vpath), ctx=cpu(0))\n",
        "        total_frames = len(vr)\n",
        "\n",
        "        # Collect all frames for all windows in this video\n",
        "        all_frames = []\n",
        "        window_info = []  # (start, num_frames_for_this_window)\n",
        "\n",
        "        for start, _ in windows:\n",
        "            frame_indices = np.linspace(\n",
        "                start,\n",
        "                min(start + cfg.num_frames - 1, total_frames - 1),\n",
        "                cfg.clip_sample_frames,\n",
        "                dtype=int\n",
        "            ).tolist()\n",
        "\n",
        "            frames = vr.get_batch(frame_indices).asnumpy()\n",
        "            all_frames.extend(list(frames))\n",
        "            window_info.append((start, len(frames)))\n",
        "\n",
        "        del vr\n",
        "\n",
        "        # Process all frames in batches\n",
        "        all_embeds = []\n",
        "        for i in range(0, len(all_frames), batch_size):\n",
        "            batch_frames = all_frames[i:i + batch_size]\n",
        "            inputs = clip_processor(images=batch_frames, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.amp.autocast('cuda', dtype=torch.float16):\n",
        "                embeds = clip_model.get_image_features(**inputs)\n",
        "            all_embeds.append(embeds)\n",
        "\n",
        "        all_embeds = torch.cat(all_embeds, dim=0)  # [total_frames, D]\n",
        "\n",
        "        # Split back by window and average pool\n",
        "        idx = 0\n",
        "        for start, num_frames in window_info:\n",
        "            window_embeds = all_embeds[idx:idx + num_frames]\n",
        "            embed = window_embeds.mean(dim=0)\n",
        "            embed = F.normalize(embed.float(), dim=-1)\n",
        "            cache[(str(vpath), start)] = embed.cpu()\n",
        "            idx += num_frames\n",
        "\n",
        "    return cache"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "trainer-header",
      "metadata": {
        "id": "trainer-header"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "Clean, minimal loop with proper PyTorch patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "trainer",
      "metadata": {
        "id": "trainer"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
        "    \"\"\"Cosine LR schedule with linear warmup.\"\"\"\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"Minimal trainer for V-JEPA → CLIP alignment.\"\"\"\n",
        "\n",
        "    def __init__(self, cfg: Config):\n",
        "        self.cfg = cfg\n",
        "        self.vjepa_device = torch.device(cfg.vjepa_device)\n",
        "        self.clip_device = torch.device(cfg.clip_device)\n",
        "\n",
        "        # Seed everything\n",
        "        torch.manual_seed(cfg.seed)\n",
        "        np.random.seed(cfg.seed)\n",
        "\n",
        "        # Initialize wandb\n",
        "        if cfg.use_wandb:\n",
        "            import wandb\n",
        "            wandb.init(\n",
        "                project=cfg.wandb_project,\n",
        "                name=cfg.wandb_run_name,\n",
        "                config=vars(cfg),\n",
        "            )\n",
        "\n",
        "        self._setup_models()\n",
        "        self._setup_data()\n",
        "        self._setup_training()\n",
        "\n",
        "    def _setup_models(self):\n",
        "        cfg = self.cfg\n",
        "        dtype = torch.bfloat16 if cfg.use_bf16 else torch.float16\n",
        "\n",
        "        # V-JEPA with PEFT LoRA\n",
        "        print(\"Loading V-JEPA...\")\n",
        "        vjepa = AutoModel.from_pretrained(\n",
        "            cfg.vjepa_model,\n",
        "            torch_dtype=dtype,\n",
        "            attn_implementation=\"sdpa\",\n",
        "        )\n",
        "\n",
        "        # Configure LoRA with PEFT\n",
        "        lora_config = LoraConfig(\n",
        "            r=cfg.lora_r,\n",
        "            lora_alpha=cfg.lora_alpha,\n",
        "            lora_dropout=cfg.lora_dropout,\n",
        "            target_modules=cfg.lora_target_modules,\n",
        "            bias=\"none\",\n",
        "            task_type=TaskType.FEATURE_EXTRACTION,\n",
        "        )\n",
        "        self.vjepa = get_peft_model(vjepa, lora_config)\n",
        "        self.vjepa.to(self.vjepa_device)\n",
        "        self.vjepa.print_trainable_parameters()\n",
        "\n",
        "        # Gradient checkpointing (with correct settings for PEFT)\n",
        "        if cfg.gradient_checkpointing:\n",
        "            self.vjepa.enable_input_require_grads()\n",
        "            self.vjepa.gradient_checkpointing_enable(\n",
        "                gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
        "            )\n",
        "            print(\"Gradient checkpointing enabled\")\n",
        "\n",
        "        # CLIP (for precomputing embeddings)\n",
        "        print(\"Loading CLIP...\")\n",
        "        self.clip = CLIPModel.from_pretrained(cfg.clip_model, torch_dtype=dtype)\n",
        "        self.clip.to(self.clip_device)\n",
        "        self.clip.eval()\n",
        "        for p in self.clip.parameters():\n",
        "            p.requires_grad_(False)\n",
        "\n",
        "        # Projection head\n",
        "        vjepa_dim = self.vjepa.config.hidden_size\n",
        "        clip_dim = self.clip.config.projection_dim\n",
        "\n",
        "        self.proj = ProjectionHead(vjepa_dim, clip_dim, cfg.proj_hidden_dim)\n",
        "        self.proj.to(self.clip_device)\n",
        "\n",
        "        if cfg.use_compile and hasattr(torch, 'compile'):\n",
        "            print(\"Compiling projection head...\")\n",
        "            self.proj = torch.compile(self.proj, mode=\"reduce-overhead\")\n",
        "\n",
        "        # Loss\n",
        "        self.criterion = InfoNCELoss(\n",
        "            temperature=cfg.temperature,\n",
        "            lambda_align=cfg.lambda_align,\n",
        "        )\n",
        "\n",
        "        # Memory bank\n",
        "        self.memory_bank = CircularMemoryBank(\n",
        "            capacity=cfg.memory_bank_size,\n",
        "            embed_dim=clip_dim,\n",
        "            device=self.clip_device,\n",
        "        )\n",
        "\n",
        "        # Processors\n",
        "        self.vjepa_processor = AutoVideoProcessor.from_pretrained(cfg.vjepa_model, use_fast=True)\n",
        "        self.clip_processor = CLIPProcessor.from_pretrained(cfg.clip_model, use_fast=True)\n",
        "\n",
        "        # Watch models with wandb\n",
        "        if cfg.use_wandb:\n",
        "            wandb.watch(self.vjepa, log='gradients', log_freq=100)\n",
        "            wandb.watch(self.proj, log='gradients', log_freq=100)\n",
        "\n",
        "    def _setup_data(self):\n",
        "        cfg = self.cfg\n",
        "        video_dir = Path(cfg.video_dir)\n",
        "\n",
        "        # Build window index\n",
        "        print(f\"Building window index from {video_dir}...\")\n",
        "        index = build_window_index(video_dir, cfg)\n",
        "        print(f\"Found {len(index)} windows\")\n",
        "\n",
        "        # CLIP cache filename based on config\n",
        "        cache_key = f\"{cfg.clip_model.replace('/', '_')}_{cfg.num_frames}f_{cfg.clip_sample_frames}cs_{cfg.window_stride}ws_{cfg.max_windows_per_video}mw_{cfg.num_videos or 'all'}v\"\n",
        "        cache_file = video_dir / f\".clip_cache_{cache_key}.pt\"\n",
        "\n",
        "        # Try to load cached embeddings\n",
        "        if cache_file.exists():\n",
        "            print(f\"Loading cached CLIP embeddings from {cache_file}\")\n",
        "            clip_cache = torch.load(cache_file, weights_only=True)\n",
        "            # Verify cache has all needed keys\n",
        "            missing = [k for k in [(str(v), s) for v, s, _ in index] if k not in clip_cache]\n",
        "            if missing:\n",
        "                print(f\"Cache missing {len(missing)} entries, recomputing...\")\n",
        "                clip_cache = None\n",
        "        else:\n",
        "            clip_cache = None\n",
        "\n",
        "        # Precompute if needed\n",
        "        if clip_cache is None:\n",
        "            print(\"Precomputing CLIP embeddings...\")\n",
        "            clip_cache = precompute_clip_embeddings(\n",
        "                index, self.clip, self.clip_processor, cfg\n",
        "            )\n",
        "            # Save cache\n",
        "            print(f\"Saving CLIP cache to {cache_file}\")\n",
        "            torch.save(clip_cache, cache_file)\n",
        "\n",
        "        # Free CLIP model memory (we only need embeddings now)\n",
        "        del self.clip\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Train/test split\n",
        "        np.random.shuffle(index)\n",
        "        split = int(len(index) * (1 - cfg.test_split))\n",
        "        train_index, test_index = index[:split], index[split:]\n",
        "\n",
        "        # Datasets\n",
        "        self.train_dataset = VideoWindowDataset(\n",
        "            train_index, self.vjepa_processor, clip_cache, cfg.num_frames\n",
        "        )\n",
        "        self.test_dataset = VideoWindowDataset(\n",
        "            test_index, self.vjepa_processor, clip_cache, cfg.num_frames\n",
        "        )\n",
        "\n",
        "        # DataLoaders with proper workers\n",
        "        self.train_loader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=cfg.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=cfg.num_workers,\n",
        "            collate_fn=collate_fn,\n",
        "            pin_memory=True,\n",
        "            persistent_workers=cfg.num_workers > 0,\n",
        "            prefetch_factor=2 if cfg.num_workers > 0 else None,\n",
        "        )\n",
        "        self.test_loader = DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=cfg.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=cfg.num_workers,\n",
        "            collate_fn=collate_fn,\n",
        "            pin_memory=True,\n",
        "            persistent_workers=cfg.num_workers > 0,\n",
        "            prefetch_factor=2 if cfg.num_workers > 0 else None,\n",
        "        )\n",
        "\n",
        "        print(f\"Train: {len(self.train_dataset)}, Test: {len(self.test_dataset)}\")\n",
        "\n",
        "    def _setup_training(self):\n",
        "        cfg = self.cfg\n",
        "\n",
        "        # Trainable params only (best practice; avoids optimizer state for frozen base)\n",
        "        self.vjepa_trainable = [p for p in self.vjepa.parameters() if p.requires_grad]\n",
        "        self.proj_trainable  = [p for p in self.proj.parameters()  if p.requires_grad]\n",
        "\n",
        "        # Separate optimizers (one per module) keeps state local to each GPU-side module\n",
        "        # and avoids cross-device gradient clipping.\n",
        "        lr_lora = getattr(cfg, \"lr_l\", cfg.lr_vjepa)\n",
        "        lr_proj = getattr(cfg, \"lr_proj\", cfg.lr_proj)\n",
        "\n",
        "        self.opt_vjepa = torch.optim.AdamW(\n",
        "            self.vjepa_trainable,\n",
        "            lr=lr_lora,\n",
        "            weight_decay=cfg.weight_decay,\n",
        "            betas=(0.9, 0.999),\n",
        "        )\n",
        "        self.opt_proj = torch.optim.AdamW(\n",
        "            self.proj_trainable,\n",
        "            lr=lr_proj,\n",
        "            weight_decay=cfg.weight_decay,\n",
        "            betas=(0.9, 0.999),\n",
        "        )\n",
        "\n",
        "        # LR schedule (same schedule shape for both)\n",
        "        num_training_steps = len(self.train_loader) * cfg.epochs\n",
        "        num_warmup_steps = int(num_training_steps * cfg.warmup_ratio)\n",
        "\n",
        "        self.sched_vjepa = get_cosine_schedule_with_warmup(\n",
        "            self.opt_vjepa, num_warmup_steps, num_training_steps\n",
        "        )\n",
        "        self.sched_proj = get_cosine_schedule_with_warmup(\n",
        "            self.opt_proj, num_warmup_steps, num_training_steps\n",
        "        )\n",
        "\n",
        "\n",
        "        # AMP\n",
        "        self.amp_dtype = torch.bfloat16 if cfg.use_bf16 else torch.float16\n",
        "\n",
        "    def train_epoch(self, epoch: int) -> dict:\n",
        "        cfg = self.cfg\n",
        "        self.vjepa.train()\n",
        "        self.proj.train()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        total_acc = 0.0\n",
        "        total_cos = 0.0\n",
        "        num_batches = 0\n",
        "        global_step = epoch * len(self.train_loader)\n",
        "\n",
        "        # EMA tracking (decay=0.99)\n",
        "        ema_decay = 0.99\n",
        "        ema_loss = None\n",
        "        ema_acc = None\n",
        "        ema_cos = None\n",
        "\n",
        "        pbar = tqdm(self.train_loader, desc=f\"Train {epoch+1}\")\n",
        "        for batch_idx, batch in enumerate(pbar):\n",
        "            pixel_values = batch['pixel_values'].to(self.vjepa_device, non_blocking=True)\n",
        "            clip_emb = batch['clip_embedding'].to(self.clip_device, non_blocking=True)\n",
        "\n",
        "            current_step = global_step + batch_idx\n",
        "\n",
        "            # Forward through V-JEPA (keep in autocast)\n",
        "            with torch.amp.autocast('cuda', dtype=self.amp_dtype):\n",
        "                outputs = self.vjepa(pixel_values_videos=pixel_values)\n",
        "                hidden = outputs.last_hidden_state  # [B, T, D] in bf16/fp16\n",
        "\n",
        "            # Pool OUTSIDE autocast and immediately convert to fp32 for stability\n",
        "            pooled = hidden.float().mean(dim=1)  # [B, D] in fp32\n",
        "\n",
        "            # Check for NaNs in V-JEPA output (skip bad batches)\n",
        "            if not torch.isfinite(pooled).all():\n",
        "                print(f\"[SKIP] Non-finite V-JEPA outputs at batch {num_batches}\")\n",
        "                # Check and reset corrupted LoRA weights\n",
        "                for name, param in self.vjepa.named_parameters():\n",
        "                    if 'lora' in name.lower() and not torch.isfinite(param).all():\n",
        "                        print(f\"  Resetting {name}\")\n",
        "                        with torch.no_grad():\n",
        "                            if 'lora_A' in name:\n",
        "                                nn.init.kaiming_uniform_(param)\n",
        "                            else:  # lora_B\n",
        "                                nn.init.zeros_(param)\n",
        "                continue\n",
        "\n",
        "            # Log V-JEPA output magnitude on first batch for monitoring\n",
        "            if num_batches == 0:\n",
        "                with torch.no_grad():\n",
        "                    print(f\"[INFO] V-JEPA pooled stats: mean={pooled.mean().item():.3f}, \"\n",
        "                          f\"std={pooled.std().item():.3f}, \"\n",
        "                          f\"min={pooled.min().item():.3f}, \"\n",
        "                          f\"max={pooled.max().item():.3f}\")\n",
        "\n",
        "            # Move to clip device and project\n",
        "            pooled = pooled.to(self.clip_device)\n",
        "\n",
        "            # Projection (fp32, no autocast)\n",
        "            with torch.amp.autocast('cuda', enabled=False):\n",
        "                projected = self.proj(pooled)\n",
        "\n",
        "                # Check projection output\n",
        "                if not torch.isfinite(projected).all():\n",
        "                    print(f\"[SKIP] Non-finite projection outputs at batch {num_batches}\")\n",
        "                    continue\n",
        "\n",
        "                # Get memory bank contents\n",
        "                memory = self.memory_bank.get_all()\n",
        "\n",
        "                # Compute loss (returns dict with combined loss + components)\n",
        "                outputs = self.criterion(projected, clip_emb, memory)\n",
        "\n",
        "            # Check if loss is valid (skip if NaN)\n",
        "            if not torch.isfinite(outputs['loss']):\n",
        "                print(f\"[SKIP] Non-finite loss\")\n",
        "                continue\n",
        "\n",
        "            # Update memory bank (after loss computation)\n",
        "            self.memory_bank.push(clip_emb.detach())\n",
        "\n",
        "            # Backward\n",
        "            self.opt_vjepa.zero_grad(set_to_none=True)\n",
        "            self.opt_proj.zero_grad(set_to_none=True)\n",
        "            outputs['loss'].backward()\n",
        "\n",
        "            # During warmup, zero out LoRA gradients (train only projection head)\n",
        "            if current_step < cfg.proj_warmup_steps:\n",
        "                for name, param in self.vjepa.named_parameters():\n",
        "                    if param.grad is not None and ('lora' in name.lower()):\n",
        "                        param.grad.zero_()\n",
        "\n",
        "            # Compute gradient stats BEFORE clipping\n",
        "            all_params = self.vjepa_trainable + self.proj_trainable\n",
        "            grads = [p.grad for p in all_params if p.grad is not None]\n",
        "\n",
        "            # Check for NaN gradients - skip this update if found\n",
        "            has_nan_grad = any(not torch.isfinite(g).all() for g in grads)\n",
        "            if has_nan_grad:\n",
        "                print(f\"[SKIP] NaN gradients at batch {num_batches}\")\n",
        "                self.opt_vjepa.zero_grad(set_to_none=True)\n",
        "                self.opt_proj.zero_grad(set_to_none=True)\n",
        "                continue\n",
        "\n",
        "            if grads:\n",
        "                # Compute norms per-tensor then combine (avoids cross-device issue)\n",
        "                grad_norms = [g.detach().norm(2).item() for g in grads]\n",
        "                grad_norm = (sum(n**2 for n in grad_norms)) ** 0.5\n",
        "                grad_max = max(g.detach().abs().max().item() for g in grads)\n",
        "            else:\n",
        "                grad_norm = 0.0\n",
        "                grad_max = 0.0\n",
        "\n",
        "            # Gradient clipping\n",
        "            if cfg.grad_clip > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(self.vjepa_trainable, cfg.grad_clip)\n",
        "                torch.nn.utils.clip_grad_norm_(self.proj_trainable, cfg.grad_clip)\n",
        "\n",
        "            self.opt_vjepa.step()\n",
        "            self.opt_proj.step()\n",
        "            self.sched_vjepa.step()\n",
        "            self.sched_proj.step()\n",
        "\n",
        "            # Track metrics\n",
        "            loss_val = outputs['loss'].item()\n",
        "            acc_val = outputs['acc'].item()\n",
        "            cos_val = outputs['cos'].item()\n",
        "\n",
        "            total_loss += loss_val\n",
        "            total_acc += acc_val\n",
        "            total_cos += cos_val\n",
        "            num_batches += 1\n",
        "\n",
        "            # Update EMAs\n",
        "            if ema_loss is None:\n",
        "                ema_loss = loss_val\n",
        "                ema_acc = acc_val\n",
        "                ema_cos = cos_val\n",
        "            else:\n",
        "                ema_loss = ema_decay * ema_loss + (1 - ema_decay) * loss_val\n",
        "                ema_acc = ema_decay * ema_acc + (1 - ema_decay) * acc_val\n",
        "                ema_cos = ema_decay * ema_cos + (1 - ema_decay) * cos_val\n",
        "\n",
        "            # Log to wandb (per step)\n",
        "            if cfg.use_wandb and num_batches % 10 == 0:\n",
        "                wandb.log({\n",
        "                    # Raw values\n",
        "                    'train/loss': loss_val,\n",
        "                    'train/contrastive': outputs['contrastive'].item(),\n",
        "                    'train/alignment': outputs['alignment'].item(),\n",
        "                    'train/acc': acc_val,\n",
        "                    'train/cos_sim': cos_val,\n",
        "                    'train/lr_proj': self.sched_proj.get_last_lr()[0],\n",
        "                    'train/lr_vjepa': self.sched_vjepa.get_last_lr()[0],\n",
        "                    'train/memory_size': self.memory_bank.size,\n",
        "                    # EMA values\n",
        "                    'train/ema_loss': ema_loss,\n",
        "                    'train/ema_acc': ema_acc,\n",
        "                    'train/ema_cos': ema_cos,\n",
        "                    # Gradient stats\n",
        "                    'grad/norm': grad_norm,\n",
        "                    'grad/max': grad_max,\n",
        "                })\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'loss': f\"{ema_loss:.3f}\",\n",
        "                'acc': f\"{ema_acc*100:.1f}%\",\n",
        "                'cos': f\"{ema_cos:.3f}\",\n",
        "                'gnorm': f\"{grad_norm:.1f}\",\n",
        "                'lr': f\"{self.sched_proj.get_last_lr()}\",\n",
        "            })\n",
        "\n",
        "        return {\n",
        "            'loss': total_loss / num_batches,\n",
        "            'acc': total_acc / num_batches,\n",
        "            'cos': total_cos / num_batches,\n",
        "        }\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, epoch: int) -> dict:\n",
        "        self.vjepa.eval()\n",
        "        self.proj.eval()\n",
        "\n",
        "        # Reset memory bank for clean eval\n",
        "        eval_memory = CircularMemoryBank(\n",
        "            capacity=self.cfg.memory_bank_size,\n",
        "            embed_dim=self.memory_bank.embed_dim,\n",
        "            device=self.clip_device,\n",
        "        )\n",
        "\n",
        "        total_loss = 0.0\n",
        "        total_acc = 0.0\n",
        "        total_cos = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch in tqdm(self.test_loader, desc=f\"Eval {epoch+1}\"):\n",
        "            pixel_values = batch['pixel_values'].to(self.vjepa_device, non_blocking=True)\n",
        "            clip_emb = batch['clip_embedding'].to(self.clip_device, non_blocking=True)\n",
        "\n",
        "            with torch.amp.autocast('cuda', dtype=self.amp_dtype):\n",
        "                outputs = self.vjepa(pixel_values_videos=pixel_values)\n",
        "                pooled = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "            pooled = pooled.to(self.clip_device)\n",
        "\n",
        "            with torch.amp.autocast('cuda', enabled=False):\n",
        "                projected = self.proj(pooled.float())\n",
        "                memory = eval_memory.get_all()\n",
        "                outputs = self.criterion(projected, clip_emb, memory)\n",
        "\n",
        "            eval_memory.push(clip_emb)\n",
        "\n",
        "            total_loss += outputs['loss'].item()\n",
        "            total_acc += outputs['acc'].item()\n",
        "            total_cos += outputs['cos'].item()\n",
        "            num_batches += 1\n",
        "\n",
        "        return {\n",
        "            'loss': total_loss / num_batches,\n",
        "            'acc': total_acc / num_batches,\n",
        "            'cos': total_cos / num_batches,\n",
        "        }\n",
        "\n",
        "    def save(self, path: str):\n",
        "        \"\"\"Save LoRA weights and projection head.\"\"\"\n",
        "        torch.save({\n",
        "            'vjepa_lora': self.vjepa.state_dict(),\n",
        "            'proj': self.proj.state_dict(),\n",
        "            'config': self.cfg,\n",
        "        }, path)\n",
        "        print(f\"Saved to {path}\")\n",
        "\n",
        "    def run(self) -> dict:\n",
        "        \"\"\"Run full training loop.\"\"\"\n",
        "        cfg = self.cfg\n",
        "        history = {'train': [], 'test': []}\n",
        "        best_loss = float('inf')\n",
        "\n",
        "        for epoch in range(cfg.epochs):\n",
        "            train_metrics = self.train_epoch(epoch)\n",
        "            test_metrics = self.evaluate(epoch)\n",
        "\n",
        "            history['train'].append(train_metrics)\n",
        "            history['test'].append(test_metrics)\n",
        "\n",
        "            # Log epoch-level metrics to wandb\n",
        "            if cfg.use_wandb:\n",
        "                wandb.log({\n",
        "                    'epoch': epoch + 1,\n",
        "                    'train/epoch_loss': train_metrics['loss'],\n",
        "                    'train/epoch_acc': train_metrics['acc'],\n",
        "                    'train/epoch_cos': train_metrics['cos'],\n",
        "                    'test/loss': test_metrics['loss'],\n",
        "                    'test/acc': test_metrics['acc'],\n",
        "                    'test/cos': test_metrics['cos'],\n",
        "                })\n",
        "\n",
        "            print(\n",
        "                f\"Epoch {epoch+1}/{cfg.epochs}: \"\n",
        "                f\"train_loss={train_metrics['loss']:.4f} \"\n",
        "                f\"test_loss={test_metrics['loss']:.4f} \"\n",
        "                f\"test_acc={test_metrics['acc']*100:.1f}%\"\n",
        "            )\n",
        "\n",
        "            if test_metrics['loss'] < best_loss:\n",
        "                best_loss = test_metrics['loss']\n",
        "                self.save('best_model.pt')\n",
        "                if cfg.use_wandb:\n",
        "                    wandb.run.summary['best_test_loss'] = best_loss\n",
        "\n",
        "        # Finish wandb run\n",
        "        if cfg.use_wandb:\n",
        "            wandb.finish()\n",
        "\n",
        "        return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "run-header",
      "metadata": {
        "id": "run-header"
      },
      "source": [
        "## Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run",
      "metadata": {
        "id": "run",
        "outputId": "38ea9f8a-b872-41c3-8e32-35e42a853dca",
        "colab": {
          "referenced_widgets": [
            "4c1101353e8044f8bbcfb3cd44feee70"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mc-daly\u001b[0m (\u001b[33mdefiant-duck\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/workspace/wandb/run-20251219_015058-dss37igw</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/defiant-duck/vjepa-clip-alignment/runs/dss37igw' target=\"_blank\">robust-frog-87</a></strong> to <a href='https://wandb.ai/defiant-duck/vjepa-clip-alignment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/defiant-duck/vjepa-clip-alignment' target=\"_blank\">https://wandb.ai/defiant-duck/vjepa-clip-alignment</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/defiant-duck/vjepa-clip-alignment/runs/dss37igw' target=\"_blank\">https://wandb.ai/defiant-duck/vjepa-clip-alignment/runs/dss37igw</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading V-JEPA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,101,248 || all params: 328,072,576 || trainable%: 0.6405\n",
            "Gradient checkpointing enabled\n",
            "Loading CLIP...\n",
            "Building window index from videos...\n",
            "Found 61817 windows\n",
            "Loading cached CLIP embeddings from videos/.clip_cache_openai_clip-vit-large-patch14_32f_8cs_4ws_256mw_allv.pt\n",
            "Train: 52544, Test: 9273\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c1101353e8044f8bbcfb3cd44feee70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Train 1:   0%|          | 0/821 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Train\n",
        "trainer = Trainer(cfg)\n",
        "history = trainer.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "plot-header",
      "metadata": {
        "id": "plot-header"
      },
      "source": [
        "## Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plot",
      "metadata": {
        "id": "plot"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(history):\n",
        "    epochs = range(1, len(history['train']) + 1)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "    # Loss\n",
        "    axes[0].plot(epochs, [h['loss'] for h in history['train']], label='train')\n",
        "    axes[0].plot(epochs, [h['loss'] for h in history['test']], label='test')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].set_title('Loss')\n",
        "\n",
        "    # Accuracy\n",
        "    axes[1].plot(epochs, [h['acc']*100 for h in history['train']], label='train')\n",
        "    axes[1].plot(epochs, [h['acc']*100 for h in history['test']], label='test')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Accuracy (%)')\n",
        "    axes[1].legend()\n",
        "    axes[1].set_title('Contrastive Accuracy')\n",
        "\n",
        "    # Cosine similarity\n",
        "    axes[2].plot(epochs, [h['cos'] for h in history['train']], label='train')\n",
        "    axes[2].plot(epochs, [h['cos'] for h in history['test']], label='test')\n",
        "    axes[2].set_xlabel('Epoch')\n",
        "    axes[2].set_ylabel('Cosine Similarity')\n",
        "    axes[2].legend()\n",
        "    axes[2].set_title('Mean Cosine Similarity')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}