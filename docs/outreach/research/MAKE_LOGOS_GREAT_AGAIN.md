# Project LOGOS — Making It Robust

This note captures the concrete additions needed for LOGOS to move from an elegant architecture to a significant research project. Treat these items as non-negotiable ingredients for any submission or public launch.

---

## 1. Benchmarks & Datasets

**Goal:** Provide replayable evidence bundles so reviewers can reproduce claims.

### Actions
1. Create `logs/benchmarks/<scenario>/` folders with:
   - Scenario README (context, goal, hardware/simulators used).
   - Replay script (`make demo_<scenario>` hook).
   - Captured artifacts (Neo4j export, Milvus dump, `/plan`+`/state` logs, persona entries, JEPA frames).
2. Annotate perception sequences with ground-truth labels (scene classifications, future frames, action outcomes).
3. Version benchmarks (timestamp + git commit) and link them from the paper draft and README.

### Why it matters
Without datasets, LOGOS remains a conceptual proposal. Benchmarks prove that the hybrid graph + CWM stack solves real tasks and give others a baseline to compare against.

---

## 1.1 What Data Matters?

LOGOS needs more than generic logs—the following artifacts make the architecture believable:

- **Scenario traces**: Canonical `CWMState` exports, `/plan` and `/state` responses, Talos telemetry, JEPA frames, and persona entries for each scenario. These show the cooperating centers in action.
- **Perception annotations**: Ground-truth labels (scene classification, pose, next-frame reference) so CWM-G predictions can be scored quantitatively.
- **Metrics outputs**: JSON summaries for latency, gullibility, coverage, and diagnostics fidelity generated by `scripts/metrics/run_all.py`.
- **Conflict/gullibility logs**: User statements that disagree with HCG predictions plus Sophia’s decision, trust delta, and recovery time. These support the gullibility metric and demonstrate robustness.
- **Cross-surface evidence**: Screenshots/videos and raw data proving the same scenario works via CLI, browser+LLM, Talos simulation, and Talos-free perception—all referencing identical `CWMState` IDs.

Every benchmark folder should explicitly include these data types or explain why a subset is irrelevant for that scenario.

### Example Benchmark Folder Layout

```
logs/benchmarks/embodied_loop/
├── README.md                 # Scenario description, hardware/simulator, commands
├── replay.sh                 # Runs the scenario end-to-end (Talos + Sophia/Hermes)
├── cwm_state.jsonl           # Appended `CWMState` events (observed + imagined)
├── plan.log                  # `/plan` responses with timestamps/latency stats
├── state.log                 # `/state` responses aligned with diagnostics
├── personas.jsonl            # Persona diary entries emitted during the run
├── jepe_frames/              # JPEG/MP4 clips from the JEPA rollout
├── talos_telemetry.jsonl     # Sensor/actuator traces (if Talos involved)
└── milvus_dump.tar.gz        # Export of embeddings referenced by this scenario
```

Use this as a reference when creating new benchmark bundles.

### Capturing Each Artifact (initial guidance)
- `cwm_state.jsonl`: `python scripts/export_cwm_state.py --scenario embodied_loop --output logs/benchmarks/embodied_loop/cwm_state.jsonl` (streams `CWMState` events from Neo4j).
- `plan.log` / `state.log`: `python scripts/metrics/record_api_traces.py --scenario embodied_loop --endpoints plan,state --output logs/benchmarks/embodied_loop`.
- `personas.jsonl`: `python scripts/export_persona.py --limit 200 --output logs/benchmarks/embodied_loop/personas.jsonl`.
- `jepe_frames/`: `python scripts/perception/export_jepa_frames.py --scenario embodied_loop --output logs/benchmarks/embodied_loop/jepe_frames`.
- `talos_telemetry.jsonl`: `talos-cli record --scenario embodied_loop --output logs/benchmarks/embodied_loop/talos_telemetry.jsonl` (replace with actual recorder when available).
- `milvus_dump.tar.gz`: `python scripts/export_milvus.py --collection scenario_embodied_loop --output logs/benchmarks/embodied_loop/milvus_dump.tar.gz`.
Document deviations or additional files in the scenario README.

See `logs/benchmarks/README.md` and the scenario-specific subfolder READMEs for detailed capture steps.

---

## 2. Automated Metrics

**Goal:** Convert the metrics ideas (latency, gullibility, scenario coverage) into executable tooling.

### Actions
1. Implement `scripts/metrics/run_all.py` to:
   - Hit Sophia/Hermes APIs with representative payloads.
   - Replay JEPA perception runs.
   - Inject gullibility/conflict test cases.
   - Compute scenario coverage scores.
2. Emit JSON summaries into `logs/metrics/<timestamp>/` and persist trends (graphs/tables).
3. Add CI workflow (`.github/workflows/metrics.yml`) to run the suite nightly or on demand.
4. Update `docs/operations/METRICS_IDEAS.md` to reference actual scripts + sample output.

### Why it matters
Quantitative evidence turns the architecture from “interesting idea” into “proven improvement.” Automated runs also guard against regressions.

### Metric Output Schema (Examples)

**Latency summary (`logs/metrics/<ts>/latency.json`):**
```json
{
  "plan": {"median_ms": 4200, "p95_ms": 5200, "samples": 50},
  "state": {"median_ms": 900, "p95_ms": 1100, "samples": 50},
  "simulate": {"median_ms": 4800, "p95_ms": 6100, "samples": 20},
  "embed_text": {"median_ms": 120, "p95_ms": 180, "samples": 100}
}
```

**Gullibility log (`logs/metrics/<ts>/gullibility.jsonl`):**
```json
{
  "conflict_id": "conflict_2024-05-11T16:30Z",
  "user_claim": "RobotArm01 already in zone B",
  "ground_truth": "HCG: RobotArm01 in zone A",
  "decision": "reject_claim",
  "trust_delta": -0.12,
  "recovery_time_s": 0
}
```

**Scenario coverage index (`logs/metrics/<ts>/scenario_coverage.json`):**
```json
{
  "embodied_loop": {"status": "pass", "evidence": "logs/scenario_coverage/embodied_loop_2024-05-10/"},
  "talos_free_perception": {"status": "pass", "evidence": "..."},
  "cli_only_ops": {"status": "fail", "reason": "planner regression"},
  "browser_llm": {"status": "pass", "evidence": "..."},
  "swappable_hardware": {"status": "pending"}
}
```

Standardizing output schemas makes it trivial to generate figures for papers or dashboards.

### Running the Metrics Suite
- Primary entry point: `python scripts/metrics/run_all.py --scenario embodied_loop --output logs/metrics/$(date -Iseconds)`.
- Individual checks:
  - Latency: `pytest tests/metrics/test_latency.py --json=logs/metrics/<ts>/latency.json`.
  - Gullibility: `python scripts/metrics/run_gullibility.py --config configs/gullibility/standard.yaml`.
  - Scenario coverage: `python scripts/metrics/update_coverage.py --scenarios scenarios/*.yaml`.
Document these commands in `scripts/metrics/README.md` so contributors can rerun subsets locally.

---

## 3. Cross-Surface Demos

**Goal:** Show that the “cooperating centers” philosophy works for multiple deployment styles.

### Scenarios (aligned with the Scenario Coverage Index)
1. **Embodied loop** — Talos (sim or real) completes a manipulation with JEPA predictions logged.
2. **Talos-free perception** — Apollo web uploads media; Sophia updates HCG without actuation.
3. **CLI-only ops** — Operator drives goal→plan→diagnostics entirely via CLI.
4. **Browser + LLM co-processor** — Chat panel proposes actions, Sophia validates, diagnostics stay consistent.
5. **Swappable hardware** — Same goal executed on two Talos adapters with identical API outputs.

### Actions
1. For each scenario, provide a `scenarios/<name>/guide.md` with inputs, commands, expected outputs/screenshots.
2. Record short demo videos or GIFs and store them in `logs/scenario_coverage/`.
3. Reference the demos in README + paper sections so readers can experience the system quickly.

### Why it matters
Concrete demos make the architecture tangible and highlight generality. They also supply qualitative evidence to complement metrics.

### Documentation Targets
- Scenario guides live under `scenarios/<name>/guide.md` and include prerequisites, commands, expected outputs, and troubleshooting notes.
- Add `make demo_<name>` targets (defined in repo root Makefile) that call the scenario’s replay script.
- Reference each guide from `README.md` (Quick Start → Scenarios) and cite them in Section 5/Section 6 of the preliminary paper.

---

## 4. Adoption & Extension Path

**Goal:** Enable other teams to extend LOGOS without breaking causal guarantees.

### Actions
1. Publish SDK or codegen instructions for Sophia/Hermes/Apollo APIs (OpenAPI → client libs) in `docs/sdk/README.md`, including sample `pip install` commands and language bindings.
2. Create `docs/EXTENDING_LOGOS.md` explaining how to add Talos adapters, perception modules, or Apollo surfaces while staying SHACL-compliant (include required node labels, relationships, and validation steps).
3. Ship validation scripts (e.g., `python scripts/validate_extension.py --adapter talos/new_driver.py`) and store sample outputs under `logs/validation/`.
4. Add governance notes (`docs/GOVERNANCE.md`) with issue templates, benchmark update process, and verification checklist expectations so contributors know how to land evidence.

### Why it matters
LOGOS matters only if others can build on it. Clear extension hooks foster adoption and demonstrate confidence in the contracts.

---

## 5. Paper Integration

**Goal:** Ensure the research paper reflects the concrete work above.

### Actions
1. Add a “Benchmark Availability” appendix in `PROJECT_LOGOS_PRELIMINARY_PAPER.md` listing scenario folders (`logs/benchmarks/<scenario>/`) and SHA references.
2. Insert metric plots/tables derived from `logs/metrics/` into Section 6 (latency, gullibility, scenario coverage).
3. Reference demo videos/screenshots (from `logs/scenario_coverage/`) in Section 5 and the evaluation discussion.
4. Summarize SDK/extension guidance (`docs/sdk/README.md`, `docs/EXTENDING_LOGOS.md`) in Section 10 (“What it takes for LOGOS to matter”).

### Paper Integration Map
| Data Source | Paper Section |
|-------------|---------------|
| `logs/benchmarks/embodied_loop/` | Section 5 (Perception & Embodiment) + Appendix A |
| `logs/benchmarks/talos_free_perception/` | Section 5 (Perception pipeline) |
| `logs/metrics/*/latency.json` | Section 6.1 (Response latency) |
| `logs/metrics/*/gullibility.jsonl` | Section 6.1 (Gullibility factor) |
| `logs/metrics/*/scenario_coverage.json` | Section 6.1 (Scenario Coverage Index) |
| Scenario guides + videos (`scenarios/*/guide.md`, `logs/scenario_coverage/*/`) | Section 5.3 (Apollo surfaces) & Evaluation |
| `docs/sdk/README.md`, `docs/EXTENDING_LOGOS.md` | Section 10 (Adoption & extension) |

---

## Summary Checklist

- [ ] Benchmarks folders + replay scripts committed.
- [ ] Metrics automation implemented and running in CI.
- [ ] Scenario demos documented with reproducible guides.
- [ ] SDK/extension guide published.
- [ ] Paper updated with benchmark + metric evidence.

When every checkbox is ticked, LOGOS graduates from an architectural blueprint to a demonstrably grounded, reusable platform—worthy of publication and external adoption.
